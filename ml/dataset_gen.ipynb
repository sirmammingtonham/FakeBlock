{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the data in a format our training script can use\n",
    "# https://www.uvic.ca/engineering/ece/isot/datasets/fake-news/index.php\n",
    "with open('../../News _dataset/Fake.csv', 'r', encoding='utf-8') as fake:\n",
    "    with open('../../News _dataset/True.csv', 'r', encoding='utf-8') as real:\n",
    "        with open('./datasets/ISOT_FakeNews.csv', 'w+', newline='', encoding='utf-8') as out:\n",
    "            fakeReader = csv.reader(fake, delimiter=',')\n",
    "            realReader = csv.reader(real, delimiter=',')\n",
    "            next(fakeReader)\n",
    "            next(realReader)\n",
    "            output = csv.writer(out, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "            output.writerow(['label', 'headline', 'text'])\n",
    "            for line in fakeReader:\n",
    "                output.writerow(['1', line[0], line[1]])\n",
    "            for line in realReader:\n",
    "                output.writerow(['0', line[0], line[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/ISOT_FakeNews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data and generate our train valid test split\n",
    "train, valid, test = np.split(df.sample(frac=1, random_state=42), [int(.6*len(df)), int(.8*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('datasets/ISOT_train.csv', index=False)\n",
    "valid.to_csv('datasets/ISOT_valid.csv', index=False)\n",
    "test.to_csv('datasets/ISOT_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-9b9f86091f19c68a\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\ethan\\.cache\\huggingface\\datasets\\csv\\default-9b9f86091f19c68a\\0.0.0\\2a88c45fed596f9421a2e7f74ab1a3cd012ef75210a5dc1950e8d60ca8d9c66c...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24b62434e6f44c159fc90f0a33ee94db"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ""
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96fa1cecce2a401fa6c0c49b2712e0ee"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ""
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0725f29ee6a34219aa266b0192d070c7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\ethan\\.cache\\huggingface\\datasets\\csv\\default-9b9f86091f19c68a\\0.0.0\\2a88c45fed596f9421a2e7f74ab1a3cd012ef75210a5dc1950e8d60ca8d9c66c. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "train_file = './datasets/ISOT_train.csv'\n",
    "eval_file = './datasets/ISOT_valid.csv'\n",
    "test_file = './datasets/ISOT_test.csv'\n",
    "files = {}\n",
    "if train_file is not None:\n",
    "    files[datasets.Split.TRAIN] = [train_file]\n",
    "if eval_file is not None:\n",
    "    files[datasets.Split.VALIDATION] = [eval_file]\n",
    "if test_file is not None:\n",
    "    files[datasets.Split.TEST] = [test_file]\n",
    "\n",
    "ds = datasets.load_dataset(\"csv\", data_files=files, quotechar='\"')\n",
    "features_name = list(ds[list(files.keys())[0]].features.keys())\n",
    "label_name = features_name.pop(0)\n",
    "label_list = list(set(ds[list(files.keys())[0]][label_name]))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "# input_names = tokenizer.model_input_names\n",
    "transformed_ds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f72a975bfb3457b9ac9171e98873ee4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'headline', 'text'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# test_ds = ds['test'].select(range(10))\n",
    "for x in test_ds:\n",
    "    print(type(x[features_name[1]]))\n",
    "\n",
    "test_ds.map(lambda x: print(type(x[features_name[1]])), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plswork(example):\n",
    "    # print(type(example[features_name[1]]))\n",
    "    try:\n",
    "        ret = tokenizer.batch_encode_plus(\n",
    "            (example[features_name[0]], example[features_name[1]]),\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # print(example[features_name[0]])\n",
    "        print(len(example[features_name[0]]))\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "batch_text_or_text_pairs has to be a list (got <class 'tuple'>)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-7542e4446fa6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m transformed_ds['test'] = ds['test'].select(range(10)).map(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mplswork\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;32mF:\\conda\\envs\\fakeblock\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[0mtest_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbatched\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[0mtest_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbatched\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1289\u001b[1;33m         \u001b[0mupdate_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoes_function_return_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1290\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Testing finished, running the mapping function on the dataset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\conda\\envs\\fakeblock\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mdoes_function_return_dict\u001b[1;34m(inputs, indices)\u001b[0m\n\u001b[0;32m   1258\u001b[0m             \u001b[0mfn_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minput_columns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m             processed_inputs = (\n\u001b[1;32m-> 1260\u001b[1;33m                 \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwith_indices\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1261\u001b[0m             )\n\u001b[0;32m   1262\u001b[0m             \u001b[0mdoes_return_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-ebffe988aca3>\u001b[0m in \u001b[0;36mplswork\u001b[1;34m(example)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# print(example[features_name[0]])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-ebffe988aca3>\u001b[0m in \u001b[0;36mplswork\u001b[1;34m(example)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# print(type(example[features_name[1]]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         ret = tokenizer.batch_encode_plus(\n\u001b[0m\u001b[0;32m      5\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\conda\\envs\\fakeblock\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2513\u001b[0m         )\n\u001b[0;32m   2514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2515\u001b[1;33m         return self._batch_encode_plus(\n\u001b[0m\u001b[0;32m   2516\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2517\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\conda\\envs\\fakeblock\\lib\\site-packages\\transformers\\tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m             raise TypeError(\n\u001b[0m\u001b[0;32m    370\u001b[0m                 \u001b[1;34m\"batch_text_or_text_pairs has to be a list (got {})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m             )\n",
      "\u001b[1;31mTypeError\u001b[0m: batch_text_or_text_pairs has to be a list (got <class 'tuple'>)"
     ]
    }
   ],
   "source": [
    "transformed_ds['test'] = ds['test'].select(range(10)).map(\n",
    "    plswork,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}